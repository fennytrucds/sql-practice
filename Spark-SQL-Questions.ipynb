{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d861701-8b09-44cf-b299-9b2ed342bdb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyspark/sql/context.py:114: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Tutorial on SparkSQL: https://github.com/cerndb/SparkTraining/blob/master/notebooks/Tutorial-SparkSQL.ipynb\n",
    "import pyspark\n",
    "# Setup the Configuration\n",
    "conf = pyspark.SparkConf()\n",
    "# Create Spark Session, you need this to work with Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder \n",
    "         .appName(\"my app\")\n",
    "         .master(\"local[1]\")\n",
    "         .config(\"spark.driver.memory\",\"1g\")\n",
    "         .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "from pyspark import SQLContext\n",
    "sc = spark.sparkContext\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aa842ed-61c2-4994-b895-b9fff39afbc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, LongType, BooleanType, TimestampType, DateType, DoubleType\n",
    "\n",
    "# If you want to use the legacy format in a newer version of spark(>3), you need to set \n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064fe02-4afd-4ab2-9707-a0b7e75474fa",
   "metadata": {},
   "source": [
    "## Question 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e30aa72-4a96-488a-89b7-beb1fcfa6793",
   "metadata": {},
   "source": [
    "> The sales department has give you the sales figures for the first two months of 2023. You've been tasked with determining the percentage of weekly sales on the first and last day of every week. Consider Sunday as last day of week and Monday as first day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798e9300-2193-487a-9d3a-2f5f37b45653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"invoicedate\", TimestampType(), True),\n",
    "    StructField(\"invoiceno\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"stockcode\", StringType(), True),\n",
    "    StructField(\"unitprice\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create the data (list of tuples)\n",
    "data = [\n",
    "    (datetime.strptime('2023-01-01 10:00:00', '%Y-%m-%d %H:%M:%S'), 1001, 5, 'A001', 20.0),\n",
    "    (datetime.strptime('2023-01-01 15:30:00', '%Y-%m-%d %H:%M:%S'), 1002, 3, 'A002', 30.0),\n",
    "    (datetime.strptime('2023-01-02 09:00:00', '%Y-%m-%d %H:%M:%S'), 1003, 10, 'A003', 15.0),\n",
    "    (datetime.strptime('2023-01-02 11:00:00', '%Y-%m-%d %H:%M:%S'), 1004, 2, 'A004', 50.0),\n",
    "    (datetime.strptime('2023-01-08 10:30:00', '%Y-%m-%d %H:%M:%S'), 1005, 4, 'A005', 25.0),\n",
    "    (datetime.strptime('2023-01-08 14:45:00', '%Y-%m-%d %H:%M:%S'), 1006, 7, 'A006', 18.0),\n",
    "    (datetime.strptime('2023-01-15 08:00:00', '%Y-%m-%d %H:%M:%S'), 1007, 6, 'A007', 22.0),\n",
    "    (datetime.strptime('2023-01-15 16:00:00', '%Y-%m-%d %H:%M:%S'), 1008, 8, 'A008', 12.0),\n",
    "    (datetime.strptime('2023-01-22 09:30:00', '%Y-%m-%d %H:%M:%S'), 1009, 3, 'A009', 40.0),\n",
    "    (datetime.strptime('2023-01-22 18:00:00', '%Y-%m-%d %H:%M:%S'), 1010, 5, 'A010', 35.0),\n",
    "    (datetime.strptime('2023-02-01 10:00:00', '%Y-%m-%d %H:%M:%S'), 1011, 9, 'A011', 20.0),\n",
    "    (datetime.strptime('2023-02-01 12:00:00', '%Y-%m-%d %H:%M:%S'), 1012, 2, 'A012', 60.0),\n",
    "    (datetime.strptime('2023-02-05 09:30:00', '%Y-%m-%d %H:%M:%S'), 1013, 4, 'A013', 25.0),\n",
    "    (datetime.strptime('2023-02-05 13:00:00', '%Y-%m-%d %H:%M:%S'), 1014, 6, 'A014', 18.0),\n",
    "    (datetime.strptime('2023-02-12 10:00:00', '%Y-%m-%d %H:%M:%S'), 1015, 7, 'A015', 22.0),\n",
    "    (datetime.strptime('2023-02-12 14:00:00', '%Y-%m-%d %H:%M:%S'), 1016, 5, 'A016', 28.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "323c9f63-eddb-4473-a009-03f4320d0b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+--------+---------+---------+\n",
      "|        invoicedate|invoiceno|quantity|stockcode|unitprice|\n",
      "+-------------------+---------+--------+---------+---------+\n",
      "|2023-01-01 10:00:00|     1001|       5|     A001|     20.0|\n",
      "|2023-01-01 15:30:00|     1002|       3|     A002|     30.0|\n",
      "|2023-01-02 09:00:00|     1003|      10|     A003|     15.0|\n",
      "|2023-01-02 11:00:00|     1004|       2|     A004|     50.0|\n",
      "|2023-01-08 10:30:00|     1005|       4|     A005|     25.0|\n",
      "+-------------------+---------+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame using the schema and data\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.createOrReplaceTempView(\"early_sales\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM early_sales LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e49c2-a9b4-4433-a545-17ee4bfe4fc9",
   "metadata": {},
   "source": [
    "I want to return the week number where Monday is the first day of the week (and Sunday is the last day), you can use weekofyear() function, but Spark SQL's default behavior considers Sunday as the start of the week.  \n",
    "\n",
    "To make Monday is the first day of the week, you can use the date_format() function along with some adjustments, as Spark SQL doesn't natively support SET DATEFIRST like SQL Server.  \n",
    "\n",
    "WEEKOFYEAR(invoicedate) AS WeekNumber, this will return the week number, but the week will start on Sunday.  \n",
    "\n",
    "To make Monday the first day of the week, we need a workaround by adjusting the date to align with ISO week system. This involves using date_format() and manipulating the weekday logic.  \n",
    "\n",
    "The 'w' format specifier gives the ISO week number in Spark SQL, where the week starts on Monday."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6abc0ff-6a96-4636-9085-6e751b40213a",
   "metadata": {},
   "source": [
    "#### Explanation to solve Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c73237-ea80-4627-bfc6-462ca1affb6d",
   "metadata": {},
   "source": [
    "1. **Weekly Sales**: This CTE calculates the total sales for each week by summming up the (quantity * unitprice) for every transaction in the given time frame (2023-01-01 to 2023-02-28). It groups by the week number (extracted from invoicedate using DATEPART)\n",
    "\n",
    "2. **SalesByDay**: This CTE calculates the daily sales for each week. It groups by both the week number and the day of the week (extracted from invoicedate). The day_of_week for Monday is 1 and for Sunday is 7.\n",
    "\n",
    "3. **FirstAndLastDatSales**: This CTE calculates the sales for Monday and Sunday of each week using conditional aggregation (CASE WHEN). It ensures that sales for Monday and Sunday are correctly captured for each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d9f24d-de9d-4450-8d27-4c4dca7b7b80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+----------------+----------------+\n",
      "|week_number|total_weekly_sales|pct_monday_sales|pct_sunday_sales|\n",
      "+-----------+------------------+----------------+----------------+\n",
      "|          1|             476.0|            53.0|            47.0|\n",
      "|          2|             228.0|             0.0|           100.0|\n",
      "|          3|             295.0|             0.0|           100.0|\n",
      "|          5|             508.0|             0.0|            41.0|\n",
      "|          6|             294.0|             0.0|           100.0|\n",
      "|         52|             190.0|             0.0|           100.0|\n",
      "+-----------+------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH WeeklySales AS (\n",
    "    SELECT \n",
    "        DATEPART('WEEK', invoicedate) as week_number,\n",
    "        SUM(quantity*unitprice) AS total_weekly_sales\n",
    "    FROM early_sales\n",
    "    WHERE invoicedate BETWEEN '2023-01-01' AND '2023-02-28'\n",
    "    GROUP BY DATEPART('WEEK', invoicedate)\n",
    "),\n",
    "SalesByDay AS (\n",
    "    SELECT\n",
    "        week_number, day_of_week,\n",
    "        SUM(sales) AS daily_sales\n",
    "    FROM\n",
    "    (\n",
    "    SELECT \n",
    "        DATEPART('WEEK', invoicedate) as week_number,\n",
    "        DATE_FORMAT(invoicedate, \"EEEE\") AS day_of_week,\n",
    "        quantity*unitprice AS sales\n",
    "    FROM early_sales\n",
    "    WHERE invoicedate BETWEEN '2023-01-01' AND '2023-02-28'\n",
    "    )\n",
    "    GROUP BY 1,2\n",
    "),\n",
    "FirstAndLastDaySales AS (\n",
    "    SELECT \n",
    "        s.week_number,\n",
    "        COALESCE(SUM(CASE WHEN s.day_of_week = \"Monday\" THEN s.daily_sales END), 0) AS monday_sales,\n",
    "        COALESCE(SUM(CASE WHEN s.day_of_week = \"Sunday\" THEN s.daily_sales END), 0) AS sunday_sales\n",
    "    FROM SalesByDay s\n",
    "    GROUP BY s.week_number\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    ws.week_number,\n",
    "    ws.total_weekly_sales,\n",
    "    ROUND(100.0 * fl.monday_sales/ws.total_weekly_sales, 0) AS pct_monday_sales,\n",
    "    ROUND(100.0 * fl.sunday_sales/ws.total_weekly_sales, 0) AS pct_sunday_sales\n",
    "FROM WeeklySales ws\n",
    "JOIN FirstAndLastDaySales fl\n",
    "ON ws.week_number = fl.week_number\n",
    "ORDER BY 1;\n",
    "\"\"\"\n",
    "\n",
    "# Run SQL query\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa85a32-7ff9-428f-8f2b-a92bd2e103a7",
   "metadata": {},
   "source": [
    "## Question 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a39494-4aea-444b-9d2b-76f2141994a1",
   "metadata": {},
   "source": [
    "> Identify users who started a session and placed an order on the same date. For these users, calculate the total number of orders and the total order value for that day. Your output should include the user, the session date, the total number of orders and the total order value for that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42c3431f-c328-495e-aeac-fae85c9b9f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define schema for sessions table\n",
    "sessions_schema = StructType([\n",
    "    StructField(\"session_id\", IntegerType(), True),\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"session_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Data for sessions table\n",
    "sessions_data = [\n",
    "    (1, 1, datetime.strptime('2024-01-01','%Y-%m-%d')),\n",
    "    (2, 2, datetime.strptime('2024-01-02','%Y-%m-%d')),\n",
    "    (3, 3, datetime.strptime('2024-01-05','%Y-%m-%d')),\n",
    "    (4, 3, datetime.strptime('2024-01-05','%Y-%m-%d')),\n",
    "    (5, 4, datetime.strptime('2024-01-03','%Y-%m-%d')),\n",
    "    (6, 4, datetime.strptime('2024-01-03','%Y-%m-%d')),\n",
    "    (7, 5, datetime.strptime('2024-01-04','%Y-%m-%d')),\n",
    "    (8, 5, datetime.strptime('2024-01-04','%Y-%m-%d')),\n",
    "    (9, 3, datetime.strptime('2024-01-05','%Y-%m-%d')),\n",
    "    (10, 5, datetime.strptime('2024-01-04','%Y-%m-%d'))\n",
    "]\n",
    "\n",
    "# Define schema for order_summary table\n",
    "order_summary_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"order_value\", IntegerType(), True),\n",
    "    StructField(\"order_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Data for order_summary table\n",
    "order_summary_data = [\n",
    "    (1, 1, 152, datetime.strptime('2024-01-01','%Y-%m-%d')),\n",
    "    (2, 2, 485, datetime.strptime('2024-01-02','%Y-%m-%d')),\n",
    "    (3, 3, 398, datetime.strptime('2024-01-05','%Y-%m-%d')),\n",
    "    (4, 3, 320, datetime.strptime('2024-01-05','%Y-%m-%d')),\n",
    "    (5, 4, 156, datetime.strptime('2024-01-03','%Y-%m-%d')),\n",
    "    (6, 4, 121, datetime.strptime('2024-01-03','%Y-%m-%d')),\n",
    "    (7, 5, 238, datetime.strptime('2024-01-04','%Y-%m-%d')),\n",
    "    (8, 5, 70, datetime.strptime('2024-01-04','%Y-%m-%d')),\n",
    "    (9, 3, 152, datetime.strptime('2024-01-05','%Y-%m-%d')),\n",
    "    (10, 5, 171, datetime.strptime('2024-01-04','%Y-%m-%d'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b82216c-0861-4de0-a99c-964ea95bc6f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create DataFrame for sessions\n",
    "sessions_df = spark.createDataFrame(sessions_data, sessions_schema)\n",
    "sessions_df.createOrReplaceTempView(\"sessions\")\n",
    "\n",
    "# Create DataFrame for order_summary\n",
    "order_summary_df = spark.createDataFrame(order_summary_data, order_summary_schema)\n",
    "order_summary_df.createOrReplaceTempView(\"order_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "595b9595-81ec-43c8-b459-36492c3ce55b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------------+\n",
      "|session_id|user_id|       session_date|\n",
      "+----------+-------+-------------------+\n",
      "|         1|      1|2024-01-01 00:00:00|\n",
      "|         2|      2|2024-01-02 00:00:00|\n",
      "|         3|      3|2024-01-05 00:00:00|\n",
      "|         4|      3|2024-01-05 00:00:00|\n",
      "|         5|      4|2024-01-03 00:00:00|\n",
      "|         6|      4|2024-01-03 00:00:00|\n",
      "|         7|      5|2024-01-04 00:00:00|\n",
      "|         8|      5|2024-01-04 00:00:00|\n",
      "|         9|      3|2024-01-05 00:00:00|\n",
      "|        10|      5|2024-01-04 00:00:00|\n",
      "+----------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sessions LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cfa70989-7380-4d39-a235-8ae6f08c2280",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------+-------------------+\n",
      "|order_id|user_id|order_value|         order_date|\n",
      "+--------+-------+-----------+-------------------+\n",
      "|       1|      1|        152|2024-01-01 00:00:00|\n",
      "|       2|      2|        485|2024-01-02 00:00:00|\n",
      "|       3|      3|        398|2024-01-05 00:00:00|\n",
      "|       4|      3|        320|2024-01-05 00:00:00|\n",
      "|       5|      4|        156|2024-01-03 00:00:00|\n",
      "|       6|      4|        121|2024-01-03 00:00:00|\n",
      "|       7|      5|        238|2024-01-04 00:00:00|\n",
      "|       8|      5|         70|2024-01-04 00:00:00|\n",
      "|       9|      3|        152|2024-01-05 00:00:00|\n",
      "|      10|      5|        171|2024-01-04 00:00:00|\n",
      "+--------+-------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM order_summary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e7434-99f4-4779-99b4-de31a79c224c",
   "metadata": {},
   "source": [
    "#### Explanation to solve query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73794b1-2ffa-4b63-9356-b3dfcd79b473",
   "metadata": {},
   "source": [
    "1. **INNER JOIN**: We're joining the sessions and order_summary tables on 2 columns session_id and user_id\n",
    "2. **COUNT**: We calculate the total number of orders placed on the same day.\n",
    "3. **SUM**: We calculate the total order value for the orders placed on the same day.\n",
    "4. **GROUP BY**: We're grouping the result by user_id and session_date to get the totals for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4c2c599-0dfb-4cc6-ba68-0df43472df31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------+------------+\n",
      "|user_id|session_date|total_orders|total_values|\n",
      "+-------+------------+------------+------------+\n",
      "|      1|  2024-01-01|           1|         152|\n",
      "|      2|  2024-01-02|           1|         485|\n",
      "|      3|  2024-01-05|           3|         870|\n",
      "|      4|  2024-01-03|           2|         277|\n",
      "|      5|  2024-01-04|           3|         479|\n",
      "+-------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    user_id, CAST(session_date AS DATE) AS session_date,\n",
    "    COUNT(DISTINCT order_id) AS total_orders,\n",
    "    SUM(order_value) AS total_values\n",
    "FROM\n",
    "(\n",
    "SELECT\n",
    "    A.session_id, A.user_id, \n",
    "    B.order_id, B.order_value,\n",
    "    A.session_date, B.order_date,\n",
    "    IF(A.session_date = B.order_date, 1, 0) AS same_date\n",
    "FROM sessions AS A\n",
    "INNER JOIN order_summary B\n",
    "ON  A.session_id = B.order_id\n",
    "    AND\n",
    "    A.user_id = B.user_id\n",
    ")\n",
    "WHERE same_date = 1\n",
    "GROUP BY 1,2\n",
    "ORDER BY 1,2\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552f0e5-3bc1-4791-a764-438ce71679c3",
   "metadata": {},
   "source": [
    "## Question 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb07534-4d14-4950-b749-9903e9b095d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "> Find all employees who have or had a job title that includes manager. Output the first name along with the corresponding title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4801d28-573d-48d4-9fea-1b2188f015d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define schema for workers table\n",
    "schema_workers = StructType([\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"joining_date\", DateType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"salary\", LongType(), True),\n",
    "    StructField(\"worker_id\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Data for workers table\n",
    "data_workers = [\n",
    "    ('HR', 'Alice', datetime.strptime('2020-01-15','%Y-%m-%d'), 'Smith', 60000, 1),\n",
    "    ('Engineering', 'Bob', datetime.strptime('2019-03-10', '%Y-%m-%d'), 'Johnson', 80000, 2),\n",
    "    ('Sales', 'Charlie', datetime.strptime('2021-07-01', '%Y-%m-%d'), 'Brown', 50000, 3),\n",
    "    ('Engineering', 'David', datetime.strptime('2018-12-20', '%Y-%m-%d'), 'Wilson', 90000, 4),\n",
    "    ('Marketing', 'Emma', datetime.strptime('2020-06-30', '%Y-%m-%d'), 'Taylor', 70000, 5)\n",
    "]\n",
    "\n",
    "# Define schema for titles table\n",
    "schema_titles = StructType([\n",
    "    StructField(\"affected_from\", DateType(), True),\n",
    "    StructField(\"worker_ref_id\", LongType(), True),\n",
    "    StructField(\"worker_title\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data for titles table\n",
    "data_titles = [\n",
    "    (datetime.strptime('2020-01-15','%Y-%m-%d'), 1, 'HR Manager'),\n",
    "    (datetime.strptime('2019-03-10','%Y-%m-%d'), 2, 'Software Engineer'),\n",
    "    (datetime.strptime('2021-07-01','%Y-%m-%d'), 3, 'Sales Representative'),\n",
    "    (datetime.strptime('2018-12-20','%Y-%m-%d'), 4, 'Engineering Manager'),\n",
    "    (datetime.strptime('2020-06-30','%Y-%m-%d'), 5, 'Marketing Specialist'),\n",
    "    (datetime.strptime('2022-01-01','%Y-%m-%d'), 5, 'Marketing Manager')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32285757-4ad1-4ed7-97c9-2aa88ef91d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create DataFrame for workers\n",
    "df_workers = spark.createDataFrame(data_workers, schema_workers)\n",
    "df_workers.createOrReplaceTempView(\"workers\")\n",
    "\n",
    "# Create DataFrame for titles\n",
    "df_titles = spark.createDataFrame(data_titles, schema_titles)\n",
    "df_titles.createOrReplaceTempView(\"titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17cef7ef-19e3-4fb7-b398-30765fd2c183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+---------+------+---------+\n",
      "| department|first_name|joining_date|last_name|salary|worker_id|\n",
      "+-----------+----------+------------+---------+------+---------+\n",
      "|         HR|     Alice|  2020-01-15|    Smith| 60000|        1|\n",
      "|Engineering|       Bob|  2019-03-10|  Johnson| 80000|        2|\n",
      "|      Sales|   Charlie|  2021-07-01|    Brown| 50000|        3|\n",
      "|Engineering|     David|  2018-12-20|   Wilson| 90000|        4|\n",
      "|  Marketing|      Emma|  2020-06-30|   Taylor| 70000|        5|\n",
      "+-----------+----------+------------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM workers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20e4fcce-c8fc-4b78-8d65-1cc9744f546f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------------------+\n",
      "|affected_from|worker_ref_id|        worker_title|\n",
      "+-------------+-------------+--------------------+\n",
      "|   2020-01-15|            1|          HR Manager|\n",
      "|   2019-03-10|            2|   Software Engineer|\n",
      "|   2021-07-01|            3|Sales Representative|\n",
      "|   2018-12-20|            4| Engineering Manager|\n",
      "|   2020-06-30|            5|Marketing Specialist|\n",
      "|   2022-01-01|            5|   Marketing Manager|\n",
      "+-------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM titles\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36e6203b-1422-4327-9a51-d48e87a2ce20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------------+\n",
      "|affected_from|worker_ref_id|       worker_title|\n",
      "+-------------+-------------+-------------------+\n",
      "|   2020-01-15|            1|         HR Manager|\n",
      "|   2018-12-20|            4|Engineering Manager|\n",
      "|   2022-01-01|            5|  Marketing Manager|\n",
      "+-------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select\n",
    "    *\n",
    "from titles\n",
    "where LOWER(worker_title) LIKE '%manager%'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf20769c-76c1-4f3c-a5af-386b0fa27bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|first_name|       worker_title|\n",
      "+----------+-------------------+\n",
      "|     Alice|         HR Manager|\n",
      "|     David|Engineering Manager|\n",
      "|      Emma|  Marketing Manager|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    B.first_name,\n",
    "    A.worker_title\n",
    "FROM\n",
    "    (\n",
    "    SELECT\n",
    "        worker_ref_id AS worker_id,\n",
    "        worker_title\n",
    "    FROM titles\n",
    "    where LOWER(worker_title) LIKE '%manager%'\n",
    "    ) AS A\n",
    "INNER JOIN workers AS B\n",
    "ON A.worker_id = B.worker_id\n",
    "\"\"\"\n",
    "\n",
    "# Run SQL query\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e81463-d129-4700-9a17-84f82a240344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3c019-38d5-41d0-a056-37e325f39551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
